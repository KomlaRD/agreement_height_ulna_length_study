{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "## Warnings\n",
    "import warnings\n",
    "## Prepare and explore data\n",
    "import pandas as pd # Data manipulation\n",
    "import numpy as np # Numeric computations\n",
    "\n",
    "## Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Split training and test data, Cross validation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Model evaluation\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor #Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # Ensemble models\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "from xgboost import XGBRegressor # Extreme gradient boostng\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder # Categorical encoder\n",
    "from sklearn.pipeline import make_pipeline # Pipeline\n",
    "from joblib import dump # Save model\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv(\"data/height_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 371 entries, 0 to 370\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          371 non-null    int64  \n",
      " 1   gender       371 non-null    object \n",
      " 2   mean_ulna    371 non-null    float64\n",
      " 3   mean_height  371 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataset structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>mean_ulna</th>\n",
       "      <th>mean_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>168.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  mean_ulna  mean_height\n",
       "0   51    male       29.0        164.3\n",
       "1   53    male       29.0        168.1\n",
       "2   69  female       24.0        161.0\n",
       "3   57    male       29.0        164.1\n",
       "4   66    male       31.0        171.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 3)\n",
      "(75, 3)\n",
      "(296,)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "# Specify features\n",
    "features = [\"age\", \"gender\", \"mean_ulna\"]\n",
    "\n",
    "# Specify target vector\n",
    "target = \"mean_height\"\n",
    "\n",
    "# Subset features\n",
    "X = df[features]\n",
    "\n",
    "# Subset target\n",
    "y = df[target]\n",
    "\n",
    "# Split dataset into 80% training and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Inspect split\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192    163.0\n",
       "75     157.8\n",
       "84     152.1\n",
       "359    156.0\n",
       "16     164.0\n",
       "Name: mean_height, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline mean absolute error 5.956830944119795\n",
      "Baseline mean squared error 54.23890199849341\n",
      "Baseline root mean squared error 7.3647065113617\n",
      "Baseline coefficient of determination 0.0\n"
     ]
    }
   ],
   "source": [
    "# Mean of the target\n",
    "y_mean = y_train.mean()\n",
    "\n",
    "# Generate baseline predictions\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "## Mean absolute error\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "print(\"Baseline mean absolute error\", mae_baseline)\n",
    "\n",
    "## Mean squared error\n",
    "mse_baseline = mean_squared_error(y_train, y_pred_baseline)\n",
    "print(\"Baseline mean squared error\", mse_baseline)\n",
    "\n",
    "## Root mean squared error\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_train, y_pred_baseline))\n",
    "print(\"Baseline root mean squared error\", rmse_baseline)\n",
    "\n",
    "# R2 score\n",
    "r2_baseline = r2_score(y_train, y_pred_baseline)\n",
    "print(\"Baseline coefficient of determination\", r2_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Linear) 3.359826019811006\n",
      "Training mean squared error (Linear) 17.722203916531388\n",
      "Training root mean squared error (Linear) 4.209774805916747\n",
      "Coefficient of determination (Linear) 0.673256587734323\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "lr_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_linear = lr_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_linear = mean_absolute_error(y_train, y_pred_linear)\n",
    "print(\"Training mean absolute error (Linear)\", mae_linear)\n",
    "\n",
    "# Mean squared error\n",
    "mse_linear = mean_squared_error(y_train, y_pred_linear)\n",
    "print(\"Training mean squared error (Linear)\", mse_linear)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_train, y_pred_linear))\n",
    "print(\"Training root mean squared error (Linear)\", rmse_linear)\n",
    "\n",
    "# R2 score\n",
    "r2_linear = r2_score(y_train, y_pred_linear)\n",
    "print(\"Coefficient of determination (Linear)\", r2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Ridge) 3.360799155092066\n",
      "Training mean squared error (Ridge) 17.722732322520113\n",
      "Training root mean squared error (Ridge) 4.2098375648616315\n",
      "Coefficient of determination (Ridge) 0.6732468455387907\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "ridge = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    Ridge(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_ridge = ridge.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_ridge = mean_absolute_error(y_train, y_pred_ridge)\n",
    "print(\"Training mean absolute error (Ridge)\", mae_ridge)\n",
    "\n",
    "# Mean squared error\n",
    "mse_ridge = mean_squared_error(y_train, y_pred_ridge)\n",
    "print(\"Training mean squared error (Ridge)\", mse_ridge)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_train, y_pred_ridge))\n",
    "print(\"Training root mean squared error (Ridge)\", rmse_ridge)\n",
    "\n",
    "# R2 score\n",
    "r2_ridge = r2_score(y_train, y_pred_ridge)\n",
    "print(\"Coefficient of determination (Ridge)\", r2_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'ridge__alpha': 1, 'ridge__solver': 'saga'}\n",
      "Best Score: 0.6509778110352706\n",
      "Training mean absolute error (Ridge) 3.3611494891012974\n",
      "Training mean squared error (Ridge) 17.72310069381754\n",
      "Training root mean squared error (Ridge) 4.209881315882615\n",
      "Coefficient of determination (Ridge) 0.6732400538950838\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Ridge regression\n",
    "ridge_parameters = {\n",
    "    'ridge__alpha': [0.1, 1, 10, 100, 1000],\n",
    "    'ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "ridge_cv = GridSearchCV(ridge, ridge_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_ridge_cv = ridge_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", ridge_cv.best_params_)\n",
    "print(\"Best Score:\", ridge_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_ridge_cv = mean_absolute_error(y_train, y_pred_ridge_cv)\n",
    "print(\"Training mean absolute error (Ridge)\", mae_ridge_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_ridge_cv = mean_squared_error(y_train, y_pred_ridge_cv)\n",
    "print(\"Training mean squared error (Ridge)\", mse_ridge_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_ridge_cv = np.sqrt(mean_squared_error(y_train, y_pred_ridge_cv))\n",
    "print(\"Training root mean squared error (Ridge)\", rmse_ridge_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_ridge_cv = r2_score(y_train, y_pred_ridge_cv)\n",
    "print(\"Coefficient of determination (Ridge)\", r2_ridge_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Lasso) 3.8676133080167543\n",
      "Training mean squared error (Lasso) 22.908404470964527\n",
      "Training root mean squared error (Lasso) 4.786272502790092\n",
      "Coefficient of determination (Lasso) 0.5776388601745506\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "lasso = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    Lasso(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_lasso = lasso.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_lasso = mean_absolute_error(y_train, y_pred_lasso)\n",
    "print(\"Training mean absolute error (Lasso)\", mae_lasso)\n",
    "\n",
    "# Mean squared error\n",
    "mse_lasso = mean_squared_error(y_train, y_pred_lasso)\n",
    "print(\"Training mean squared error (Lasso)\", mse_lasso)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_train, y_pred_lasso))\n",
    "print(\"Training root mean squared error (Lasso)\", rmse_lasso)\n",
    "\n",
    "# R2 score\n",
    "r2_lasso = r2_score(y_train, y_pred_lasso)\n",
    "print(\"Coefficient of determination (Lasso)\", r2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lasso__alpha': 0.01}\n",
      "Best Score: 0.6508809302002576\n",
      "Training mean absolute error (Lasso) 3.3607827795791265\n",
      "Training mean squared error (Lasso) 17.72275252317157\n",
      "Training root mean squared error (Lasso) 4.2098399640807695\n",
      "Coefficient of determination (Lasso) 0.6732464731003616\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Lasso model\n",
    "lasso_parameters = {'lasso__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    \n",
    "lasso_cv = GridSearchCV(lasso, lasso_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_lasso_cv = lasso_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", lasso_cv.best_params_)\n",
    "print(\"Best Score:\", lasso_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_lasso_cv = mean_absolute_error(y_train, y_pred_lasso_cv)\n",
    "print(\"Training mean absolute error (Lasso)\", mae_lasso_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_lasso_cv = mean_squared_error(y_train, y_pred_lasso_cv)\n",
    "print(\"Training mean squared error (Lasso)\", mse_lasso_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_lasso_cv = np.sqrt(mean_squared_error(y_train, y_pred_lasso_cv))\n",
    "print(\"Training root mean squared error (Lasso)\", rmse_lasso_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_lasso_cv = r2_score(y_train, y_pred_lasso_cv)\n",
    "print(\"Coefficient of determination (Lasso)\", r2_lasso_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (ElasticNet) 3.7217925323600856\n",
      "Training mean squared error (ElasticNet) 21.352749653892875\n",
      "Training root mean squared error (ElasticNet) 4.620903553840187\n",
      "Coefficient of determination (Elastic) 0.6063203924281877\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "elastic = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    ElasticNet(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_elastic = elastic.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_elastic = mean_absolute_error(y_train, y_pred_elastic)\n",
    "print(\"Training mean absolute error (ElasticNet)\", mae_elastic)\n",
    "\n",
    "# Mean squared error\n",
    "mse_elastic = mean_squared_error(y_train, y_pred_elastic)\n",
    "print(\"Training mean squared error (ElasticNet)\", mse_elastic)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_elastic = np.sqrt(mean_squared_error(y_train, y_pred_elastic))\n",
    "print(\"Training root mean squared error (ElasticNet)\", rmse_elastic)\n",
    "\n",
    "# R2 score\n",
    "r2_elastic = r2_score(y_train, y_pred_elastic)\n",
    "print(\"Coefficient of determination (Elastic)\", r2_elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.344e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.006e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.031e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.344e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.006e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.432e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.269e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.809e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.809e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'elasticnet__alpha': 0.01, 'elasticnet__l1_ratio': 0.0, 'elasticnet__selection': 'cyclic'}\n",
      "Best Score: 0.651054776153045\n",
      "Training mean absolute error (ElasticNet) 3.3626317420104828\n",
      "Training mean squared error (ElasticNet) 17.726639497392146\n",
      "Training root mean squared error (ElasticNet) 4.2103015922130975\n",
      "Coefficient of determination (Elastic) 0.6731748091455735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+03, tolerance: 1.605e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV for ElasticNet\n",
    "elastic_parameters = {\n",
    "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'elasticnet__l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    'elasticnet__selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "elastic_cv = GridSearchCV(elastic, elastic_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "elastic_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_elastic_cv = elastic_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", elastic_cv.best_params_)\n",
    "print(\"Best Score:\", elastic_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_elastic_cv = mean_absolute_error(y_train, y_pred_elastic_cv)\n",
    "print(\"Training mean absolute error (ElasticNet)\", mae_elastic_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_elastic_cv = mean_squared_error(y_train, y_pred_elastic_cv)\n",
    "print(\"Training mean squared error (ElasticNet)\", mse_elastic_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_elastic_cv = np.sqrt(mean_squared_error(y_train, y_pred_elastic_cv))\n",
    "print(\"Training root mean squared error (ElasticNet)\", rmse_elastic_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_elastic_cv = r2_score(y_train, y_pred_elastic_cv)\n",
    "print(\"Coefficient of determination (Elastic)\", r2_elastic_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Random Forest) 1.6993276034963478\n",
      "Training mean squared error (Random Forest) 5.15115593898109\n",
      "Training root mean squared error (Random Forest) 2.269615813079626\n",
      "Coefficient of determination (Random Forest) 0.9050283883120611\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "rf_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    RandomForestRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_rf = rf_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_rf = mean_absolute_error(y_train, y_pred_rf)\n",
    "print(\"Training mean absolute error (Random Forest)\", mae_rf)\n",
    "\n",
    "# Mean squared error\n",
    "mse_rf = mean_squared_error(y_train, y_pred_rf)\n",
    "print(\"Training mean squared error (Random Forest)\", mse_rf)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_train, y_pred_rf))\n",
    "print(\"Training root mean squared error (Random Forest)\", rmse_rf)\n",
    "\n",
    "# R2 score\n",
    "r2_rf = r2_score(y_train, y_pred_rf)\n",
    "print(\"Coefficient of determination (Random Forest)\", r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'randomforestregressor__criterion': 'squared_error', 'randomforestregressor__max_depth': 10, 'randomforestregressor__max_features': 'sqrt', 'randomforestregressor__max_leaf_nodes': 10, 'randomforestregressor__n_estimators': 300}\n",
      "Best Score: 0.6013956612742823\n",
      "Training mean absolute error (Random Forest) 3.204513929282441\n",
      "Training mean squared error (Random Forest) 16.251055326582144\n",
      "Training root mean squared error (Random Forest) 4.031259769176645\n",
      "Coefficient of determination (Random Forest) 0.7003800827857181\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV with random forest\n",
    "\n",
    "rf_parameters = {\n",
    "    'randomforestregressor__n_estimators': [100, 200, 300],\n",
    "    'randomforestregressor__max_features': ['sqrt', 'log2', None],\n",
    "    'randomforestregressor__max_depth': [10, 20, 30, None],\n",
    "    'randomforestregressor__max_leaf_nodes': [None, 10, 20, 50, 100],\n",
    "    'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf_model, rf_parameters, cv=5, n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_rf_cv = rf_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", rf_cv.best_params_)\n",
    "print(\"Best Score:\", rf_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_rf_cv = mean_absolute_error(y_train, y_pred_rf_cv)\n",
    "print(\"Training mean absolute error (Random Forest)\", mae_rf_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_rf_cv = mean_squared_error(y_train, y_pred_rf_cv)\n",
    "print(\"Training mean squared error (Random Forest)\", mse_rf_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_rf_cv = np.sqrt(mean_squared_error(y_train, y_pred_rf_cv))\n",
    "print(\"Training root mean squared error (Random Forest)\", rmse_rf_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_rf_cv = r2_score(y_train, y_pred_rf_cv)\n",
    "print(\"Coefficient of determination (Random Forest)\", r2_rf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Gradient Boosting) 2.5354934838250602\n",
      "Training mean squared error (Gradient Boosting) 10.40970268118234\n",
      "Training root mean squared error (Gradient Boosting) 3.226407085471754\n",
      "Coefficient of determination (Gradient Boosting) 0.8080768176046136\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "gb_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    GradientBoostingRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_gb = gb_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_gb = mean_absolute_error(y_train, y_pred_gb)\n",
    "print(\"Training mean absolute error (Gradient Boosting)\", mae_gb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_gb = mean_squared_error(y_train, y_pred_gb)\n",
    "print(\"Training mean squared error (Gradient Boosting)\", mse_gb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_train, y_pred_gb))\n",
    "print(\"Training root mean squared error (Gradient Boosting)\", rmse_gb)\n",
    "\n",
    "# R2 score\n",
    "r2_gb = r2_score(y_train, y_pred_gb)\n",
    "print(\"Coefficient of determination (Gradient Boosting)\", r2_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'gradientboostingregressor__learning_rate': 0.01, 'gradientboostingregressor__loss': 'squared_error', 'gradientboostingregressor__max_depth': 3, 'gradientboostingregressor__n_estimators': 300, 'gradientboostingregressor__subsample': 0.8}\n",
      "Best Score: 0.6036805893117679\n",
      "Training mean absolute error (Gradient Boosting) 3.009678113851897\n",
      "Training mean squared error (Gradient Boosting) 14.256709760586736\n",
      "Training root mean squared error (Gradient Boosting) 3.7758058425436465\n",
      "Coefficient of determination (Gradient Boosting) 0.7371497350557955\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV with gradient boosting\n",
    "\n",
    "gb_parameters = {\n",
    "    'gradientboostingregressor__n_estimators': [100, 200, 300],\n",
    "    'gradientboostingregressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboostingregressor__max_depth': [3, 4, 5],\n",
    "    'gradientboostingregressor__subsample': [0.8, 0.9, 1.0],\n",
    "    'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "}\n",
    "\n",
    "\n",
    "gb_cv = GridSearchCV(gb_model, gb_parameters, cv=5, n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "gb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_gb_cv = gb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", gb_cv.best_params_)\n",
    "print(\"Best Score:\", gb_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_gb_cv = mean_absolute_error(y_train, y_pred_gb_cv)\n",
    "print(\"Training mean absolute error (Gradient Boosting)\", mae_gb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_gb_cv = mean_squared_error(y_train, y_pred_gb_cv)\n",
    "print(\"Training mean squared error (Gradient Boosting)\", mse_gb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_gb_cv = np.sqrt(mean_squared_error(y_train, y_pred_gb_cv))\n",
    "print(\"Training root mean squared error (Gradient Boosting)\", rmse_gb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_gb_cv = r2_score(y_train, y_pred_gb_cv)\n",
    "print(\"Coefficient of determination (Gradient Boosting)\", r2_gb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Extreme Gradient) 0.6843069437387823\n",
      "Training mean squared error (Extreme Gradient) 2.359513918332397\n",
      "Training root mean squared error (Extreme Gradient) 1.5360709353191984\n",
      "Coefficient of determination (Extreme Gradien) 0.9564977565659804\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "xgb_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    XGBRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_xgb = xgb_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_xgb = mean_absolute_error(y_train, y_pred_xgb)\n",
    "print(\"Training mean absolute error (Extreme Gradient)\", mae_xgb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_xgb = mean_squared_error(y_train, y_pred_xgb)\n",
    "print(\"Training mean squared error (Extreme Gradient)\", mse_xgb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_train, y_pred_xgb))\n",
    "print(\"Training root mean squared error (Extreme Gradient)\", rmse_xgb)\n",
    "\n",
    "# R2 score\n",
    "r2_xgb = r2_score(y_train, y_pred_xgb)\n",
    "print(\"Coefficient of determination (Extreme Gradien)\", r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'xgbregressor__colsample_bytree': 0.7, 'xgbregressor__gamma': 0.2, 'xgbregressor__learning_rate': 0.01, 'xgbregressor__max_depth': 3, 'xgbregressor__min_child_weight': 5, 'xgbregressor__n_estimators': 300, 'xgbregressor__subsample': 0.8}\n",
      "Best Score: 0.6119288282528219\n",
      "Training mean absolute error (Extreme Gradient) 3.1936014845564555\n",
      "Training mean squared error (Extreme Gradient) 16.44130288655992\n",
      "Training root mean squared error (Extreme Gradient) 4.05478764999598\n",
      "Coefficient of determination (Extreme Gradient) 0.6968724977689149\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV with extreme gradient boosting\n",
    "xgb_parameters = {\n",
    "    'xgbregressor__n_estimators': [100, 200, 300],\n",
    "    'xgbregressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgbregressor__max_depth': [3, 4, 5],\n",
    "    'xgbregressor__subsample': [0.8, 0.9, 1.0],\n",
    "    'xgbregressor__colsample_bytree': [0.3, 0.7, 1.0],\n",
    "    'xgbregressor__min_child_weight': [1, 3, 5],\n",
    "    'xgbregressor__gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_cv = GridSearchCV(xgb_model, xgb_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_xgb_cv = xgb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", xgb_cv.best_params_)\n",
    "print(\"Best Score:\", xgb_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_xgb_cv = mean_absolute_error(y_train, y_pred_xgb_cv)\n",
    "print(\"Training mean absolute error (Extreme Gradient)\", mae_xgb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_xgb_cv = mean_squared_error(y_train, y_pred_xgb_cv)\n",
    "print(\"Training mean squared error (Extreme Gradient)\", mse_xgb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_xgb_cv = np.sqrt(mean_squared_error(y_train, y_pred_xgb_cv))\n",
    "print(\"Training root mean squared error (Extreme Gradient)\", rmse_xgb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_xgb_cv = r2_score(y_train, y_pred_xgb_cv)\n",
    "print(\"Coefficient of determination (Extreme Gradient)\", r2_xgb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Decision Tree) 0.49566441441441444\n",
      "Training mean squared error (Decision Tree) 2.263623310810811\n",
      "Training root mean squared error (Decision Tree) 1.5045342504611887\n",
      "Coefficient of determination (Decision Tree) 0.9582656870363325\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "tree_model = make_pipeline(\n",
    "    OrdinalEncoder(),\n",
    "    DecisionTreeRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_tree = tree_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_tree = mean_absolute_error(y_train, y_pred_tree)\n",
    "print(\"Training mean absolute error (Decision Tree)\", mae_tree)\n",
    "\n",
    "# Mean squared error\n",
    "mse_tree = mean_squared_error(y_train, y_pred_tree)\n",
    "print(\"Training mean squared error (Decision Tree)\", mse_tree)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_train, y_pred_tree))\n",
    "print(\"Training root mean squared error (Decision Tree)\", rmse_tree)\n",
    "\n",
    "# R2 score\n",
    "r2_tree = r2_score(y_train, y_pred_tree)\n",
    "print(\"Coefficient of determination (Decision Tree)\", r2_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'decisiontreeregressor__max_depth': 10, 'decisiontreeregressor__max_features': 'auto', 'decisiontreeregressor__max_leaf_nodes': None, 'decisiontreeregressor__min_samples_leaf': 2, 'decisiontreeregressor__min_samples_split': 10, 'decisiontreeregressor__splitter': 'random'}\n",
      "Best Score: 0.5660823562865834\n",
      "Training mean absolute error (Decision Tree) 3.1373784110197147\n",
      "Training mean squared error (Decision Tree) 15.349416943535147\n",
      "Training root mean squared error (Decision Tree) 3.9178331949605956\n",
      "Coefficient of determination (Decision Tree) 0.7170035458320763\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV with decision tree\n",
    "tree_parameters = {\n",
    "    'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "    'decisiontreeregressor__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'decisiontreeregressor__min_samples_split': [2, 5, 10],\n",
    "    'decisiontreeregressor__min_samples_leaf': [1, 2, 4],\n",
    "    'decisiontreeregressor__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'decisiontreeregressor__max_leaf_nodes': [None, 10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "\n",
    "tree_cv = GridSearchCV(tree_model, tree_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_tree_cv = tree_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", tree_cv.best_params_)\n",
    "print(\"Best Score:\", tree_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_tree_cv = mean_absolute_error(y_train, y_pred_tree_cv)\n",
    "print(\"Training mean absolute error (Decision Tree)\", mae_tree_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_tree_cv = mean_squared_error(y_train, y_pred_tree_cv)\n",
    "print(\"Training mean squared error (Decision Tree)\", mse_tree_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_tree_cv = np.sqrt(mean_squared_error(y_train, y_pred_tree_cv))\n",
    "print(\"Training root mean squared error (Decision Tree)\", rmse_tree_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_tree_cv = r2_score(y_train, y_pred_tree_cv)\n",
    "print(\"Coefficient of determination (Decision Tree)\", r2_tree_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Huber) 3.351836159007122\n",
      "Training mean squared error (Huber) 17.773416256596814\n",
      "Training root mean squared error (Huber) 4.215852969043965\n",
      "Coefficient of determination (Huber) 0.6723123883095845\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "hb = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    HuberRegressor()\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "hb.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_hb = hb.predict(X_train)\n",
    "\n",
    "# Evaluate predictions\n",
    "# Mean absolute error\n",
    "mae_hb = mean_absolute_error(y_train, y_pred_hb)\n",
    "print(\"Training mean absolute error (Huber)\", mae_hb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_hb = mean_squared_error(y_train, y_pred_hb)\n",
    "print(\"Training mean squared error (Huber)\", mse_hb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_hb = np.sqrt(mean_squared_error(y_train, y_pred_hb))\n",
    "print(\"Training root mean squared error (Huber)\", rmse_hb)\n",
    "\n",
    "# R2 score\n",
    "r2_hb = r2_score(y_train, y_pred_hb)\n",
    "print(\"Coefficient of determination (Huber)\", r2_hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'huberregressor__alpha': 1, 'huberregressor__epsilon': 1.75, 'huberregressor__max_iter': 100}\n",
      "Best Score: 0.6508643341446835\n",
      "Training mean absolute error (Huber) 3.358655804803407\n",
      "Training mean squared error (Huber) 17.737431953070505\n",
      "Training root mean squared error (Huber) 4.211583069710309\n",
      "Coefficient of determination (Huber) 0.6729758291647719\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV with Huber regressor\n",
    "hb_parameters = {\n",
    "    'huberregressor__epsilon': [1.35, 1.5, 1.75, 2.0],\n",
    "    'huberregressor__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'huberregressor__max_iter': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "hb_cv = GridSearchCV(hb, hb_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "hb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_hb_cv = hb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", hb_cv.best_params_)\n",
    "print(\"Best Score:\", hb_cv.best_score_)\n",
    "\n",
    "# Evaluate predictions\n",
    "# Mean absolute error\n",
    "mae_hb_cv = mean_absolute_error(y_train, y_pred_hb_cv)\n",
    "print(\"Training mean absolute error (Huber)\", mae_hb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_hb_cv = mean_squared_error(y_train, y_pred_hb_cv)\n",
    "print(\"Training mean squared error (Huber)\", mse_hb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_hb_cv = np.sqrt(mean_squared_error(y_train, y_pred_hb_cv))\n",
    "print(\"Training root mean squared error (Huber)\", rmse_hb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_hb_cv = r2_score(y_train, y_pred_hb_cv)\n",
    "print(\"Coefficient of determination (Huber)\", r2_hb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(models, x, y):\n",
    "    \"\"\"\n",
    "    Evaluate models on test data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models: dict, Dictionary with instances of models\n",
    "    x: DataFrame, Feature matrix for test data\n",
    "    y: DataFrame, Target vector for test data\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Results of model evaluation: DataFrame, Name of model, mean absolute error, mean squared error, \n",
    "    root mean squared error, coefficient of determination\n",
    "    \"\"\"\n",
    "    # Empty list to hold results\n",
    "    test_results = []\n",
    "    \n",
    "    # Loop through model dictionary\n",
    "    for name, model in models.items():\n",
    "        # Create a prediction series with model\n",
    "        y_pred_test = model.predict(x)\n",
    "        \n",
    "        # Evaluate mean absolute error\n",
    "        mae = mean_absolute_error(y, y_pred_test)\n",
    "        \n",
    "        # Evaluate mean squared error\n",
    "        mse = mean_squared_error(y, y_pred_test)\n",
    "        \n",
    "        # Evaluate root mean squared error\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred_test))\n",
    "        \n",
    "        # Evaluate coefficient of determination\n",
    "        r2 = r2_score(y, y_pred_test)\n",
    "        \n",
    "        # Append name of model and model metrics\n",
    "        test_results.append((name, mae, mse, rmse, r2))\n",
    "\n",
    "    # Create a dataframe with test result list\n",
    "    results_df = pd.DataFrame(test_results, columns=[\"Models\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"])\n",
    "    \n",
    "    # Return data frame of test results\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear regression</td>\n",
       "      <td>3.531969</td>\n",
       "      <td>20.259276</td>\n",
       "      <td>4.501031</td>\n",
       "      <td>0.597047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge regression</td>\n",
       "      <td>3.533785</td>\n",
       "      <td>20.266195</td>\n",
       "      <td>4.501799</td>\n",
       "      <td>0.596910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>3.533476</td>\n",
       "      <td>20.262739</td>\n",
       "      <td>4.501415</td>\n",
       "      <td>0.596978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elastic net</td>\n",
       "      <td>3.536202</td>\n",
       "      <td>20.274793</td>\n",
       "      <td>4.502754</td>\n",
       "      <td>0.596739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>3.689465</td>\n",
       "      <td>20.956289</td>\n",
       "      <td>4.577804</td>\n",
       "      <td>0.583184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>3.534766</td>\n",
       "      <td>19.644300</td>\n",
       "      <td>4.432189</td>\n",
       "      <td>0.609279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB</td>\n",
       "      <td>3.557643</td>\n",
       "      <td>19.894632</td>\n",
       "      <td>4.460340</td>\n",
       "      <td>0.604300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>3.522003</td>\n",
       "      <td>19.629114</td>\n",
       "      <td>4.430476</td>\n",
       "      <td>0.609581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Huber regressor</td>\n",
       "      <td>3.534908</td>\n",
       "      <td>20.213917</td>\n",
       "      <td>4.495989</td>\n",
       "      <td>0.597949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Models       MAE        MSE      RMSE        R2\n",
       "0  Linear regression  3.531969  20.259276  4.501031  0.597047\n",
       "1   Ridge regression  3.533785  20.266195  4.501799  0.596910\n",
       "2              Lasso  3.533476  20.262739  4.501415  0.596978\n",
       "3        Elastic net  3.536202  20.274793  4.502754  0.596739\n",
       "4      Random Forest  3.689465  20.956289  4.577804  0.583184\n",
       "5  Gradient Boosting  3.534766  19.644300  4.432189  0.609279\n",
       "6                XGB  3.557643  19.894632  4.460340  0.604300\n",
       "7      Decision tree  3.522003  19.629114  4.430476  0.609581\n",
       "8    Huber regressor  3.534908  20.213917  4.495989  0.597949"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Linear regression\": lr_model,\n",
    "    \"Ridge regression\": ridge_cv,\n",
    "    \"Lasso\": lasso_cv,\n",
    "    \"Elastic net\": elastic_cv,\n",
    "    \"Random Forest\": rf_cv,\n",
    "    \"Gradient Boosting\": gb_cv,\n",
    "    \"XGB\": xgb_cv,\n",
    "    \"Decision tree\": tree_cv,\n",
    "    \"Huber regressor\": hb\n",
    "}\n",
    "\n",
    "# Call test model with args: models, X_test, y_test\n",
    "test_model(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>mean_ulna</th>\n",
       "      <th>mean_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1033</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>164.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1091</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>29.3</td>\n",
       "      <td>172.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1109</td>\n",
       "      <td>19</td>\n",
       "      <td>male</td>\n",
       "      <td>25.0</td>\n",
       "      <td>155.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1037</td>\n",
       "      <td>20</td>\n",
       "      <td>male</td>\n",
       "      <td>27.0</td>\n",
       "      <td>168.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1075</td>\n",
       "      <td>20</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID  age gender  mean_ulna  mean_height\n",
       "0  1033   19   male       28.0        164.5\n",
       "1  1091   19   male       29.3        172.0\n",
       "2  1109   19   male       25.0        155.6\n",
       "3  1037   20   male       27.0        168.0\n",
       "4  1075   20   male       30.0        171.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# External data\n",
    "df_test = pd.read_csv(\"data/test_set.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"mean_height\"\n",
    "features = [\"age\", \"gender\", \"mean_ulna\"]\n",
    "\n",
    "X_test_ext = df_test[features]\n",
    "y_test_ext = df_test[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Models</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear regression</td>\n",
       "      <td>4.597605</td>\n",
       "      <td>32.729807</td>\n",
       "      <td>5.720997</td>\n",
       "      <td>0.592504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge regression</td>\n",
       "      <td>4.600713</td>\n",
       "      <td>32.747310</td>\n",
       "      <td>5.722527</td>\n",
       "      <td>0.592286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>4.600117</td>\n",
       "      <td>32.748036</td>\n",
       "      <td>5.722590</td>\n",
       "      <td>0.592277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elastic net</td>\n",
       "      <td>4.605889</td>\n",
       "      <td>32.779077</td>\n",
       "      <td>5.725301</td>\n",
       "      <td>0.591890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>4.833981</td>\n",
       "      <td>37.197013</td>\n",
       "      <td>6.098935</td>\n",
       "      <td>0.536885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>4.757346</td>\n",
       "      <td>34.947063</td>\n",
       "      <td>5.911604</td>\n",
       "      <td>0.564898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB</td>\n",
       "      <td>4.892192</td>\n",
       "      <td>37.744984</td>\n",
       "      <td>6.143695</td>\n",
       "      <td>0.530063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision tree</td>\n",
       "      <td>5.026712</td>\n",
       "      <td>39.171954</td>\n",
       "      <td>6.258750</td>\n",
       "      <td>0.512297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Huber regressor</td>\n",
       "      <td>4.581499</td>\n",
       "      <td>32.797521</td>\n",
       "      <td>5.726912</td>\n",
       "      <td>0.591661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Models       MAE        MSE      RMSE        R2\n",
       "0  Linear regression  4.597605  32.729807  5.720997  0.592504\n",
       "1   Ridge regression  4.600713  32.747310  5.722527  0.592286\n",
       "2              Lasso  4.600117  32.748036  5.722590  0.592277\n",
       "3        Elastic net  4.605889  32.779077  5.725301  0.591890\n",
       "4      Random Forest  4.833981  37.197013  6.098935  0.536885\n",
       "5  Gradient Boosting  4.757346  34.947063  5.911604  0.564898\n",
       "6                XGB  4.892192  37.744984  6.143695  0.530063\n",
       "7      Decision tree  5.026712  39.171954  6.258750  0.512297\n",
       "8    Huber regressor  4.581499  32.797521  5.726912  0.591661"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(models, X_test_ext, y_test_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "female    267\n",
       "male      104\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gender\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['linear_model.joblib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save linear regression model\n",
    "dump(models[\"Linear regression\"][\"linearregression\"], 'linear_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predicting_height",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
