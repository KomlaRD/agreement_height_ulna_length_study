{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "## Warnings\n",
    "import warnings\n",
    "## Prepare and explore data\n",
    "import pandas as pd # Data manipulation\n",
    "import numpy as np # Numeric computations\n",
    "\n",
    "## Machine learning packages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV # Split training and test data, Cross validation\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Model evaluation\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor #Linear models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor # Ensemble models\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree\n",
    "from xgboost import XGBRegressor # Extreme gradient boostng\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder # Categorical encoder\n",
    "from sklearn.pipeline import make_pipeline # Pipeline\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data\n",
    "## Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "df = pd.read_csv(\"data/height_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 371 entries, 0 to 370\n",
      "Data columns (total 4 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          371 non-null    int64  \n",
      " 1   gender       371 non-null    object \n",
      " 2   mean_ulna    371 non-null    float64\n",
      " 3   mean_height  371 non-null    float64\n",
      "dtypes: float64(2), int64(1), object(1)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Dataset structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>mean_ulna</th>\n",
       "      <th>mean_height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>168.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69</td>\n",
       "      <td>female</td>\n",
       "      <td>24.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>57</td>\n",
       "      <td>male</td>\n",
       "      <td>29.0</td>\n",
       "      <td>164.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>66</td>\n",
       "      <td>male</td>\n",
       "      <td>31.0</td>\n",
       "      <td>171.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  gender  mean_ulna  mean_height\n",
       "0   51    male       29.0        164.3\n",
       "1   53    male       29.0        168.1\n",
       "2   69  female       24.0        161.0\n",
       "3   57    male       29.0        164.1\n",
       "4   66    male       31.0        171.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first five rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(296, 3)\n",
      "(75, 3)\n",
      "(296,)\n",
      "(75,)\n"
     ]
    }
   ],
   "source": [
    "# Specify features\n",
    "features = [\"age\", \"gender\", \"mean_ulna\"]\n",
    "\n",
    "# Specify target vector\n",
    "target = \"mean_height\"\n",
    "\n",
    "# Subset features\n",
    "X = df[features]\n",
    "\n",
    "# Subset target\n",
    "y = df[target]\n",
    "\n",
    "# Split dataset into 80% training and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# Inspect split\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192    163.0\n",
       "75     157.8\n",
       "84     152.1\n",
       "359    156.0\n",
       "16     164.0\n",
       "Name: mean_height, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model\n",
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline mean absolute error 5.956830944119795\n",
      "Baseline mean squared error 54.23890199849341\n",
      "Baseline root mean squared error 7.3647065113617\n",
      "Baseline coefficient of determination 0.0\n"
     ]
    }
   ],
   "source": [
    "# Mean of the target\n",
    "y_mean = y_train.mean()\n",
    "\n",
    "# Generate baseline predictions\n",
    "y_pred_baseline = [y_mean] * len(y_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "## Mean absolute error\n",
    "mae_baseline = mean_absolute_error(y_train, y_pred_baseline)\n",
    "print(\"Baseline mean absolute error\", mae_baseline)\n",
    "\n",
    "## Mean squared error\n",
    "mse_baseline = mean_squared_error(y_train, y_pred_baseline)\n",
    "print(\"Baseline mean squared error\", mse_baseline)\n",
    "\n",
    "## Root mean squared error\n",
    "rmse_baseline = np.sqrt(mean_squared_error(y_train, y_pred_baseline))\n",
    "print(\"Baseline root mean squared error\", rmse_baseline)\n",
    "\n",
    "# R2 score\n",
    "r2_baseline = r2_score(y_train, y_pred_baseline)\n",
    "print(\"Baseline coefficient of determination\", r2_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Linear) 3.3598260198110053\n",
      "Training mean squared error (Linear) 17.72220391653138\n",
      "Training root mean squared error (Linear) 4.209774805916746\n",
      "Coefficient of determination (Linear) 0.6732565877343231\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "lr_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    LinearRegression()\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_linear = lr_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_linear = mean_absolute_error(y_train, y_pred_linear)\n",
    "print(\"Training mean absolute error (Linear)\", mae_linear)\n",
    "\n",
    "# Mean squared error\n",
    "mse_linear = mean_squared_error(y_train, y_pred_linear)\n",
    "print(\"Training mean squared error (Linear)\", mse_linear)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_linear = np.sqrt(mean_squared_error(y_train, y_pred_linear))\n",
    "print(\"Training root mean squared error (Linear)\", rmse_linear)\n",
    "\n",
    "# R2 score\n",
    "r2_linear = r2_score(y_train, y_pred_linear)\n",
    "print(\"Coefficient of determination (Linear)\", r2_linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Ridge) 3.360799155092066\n",
      "Training mean squared error (Ridge) 17.722732322520113\n",
      "Training root mean squared error (Ridge) 4.2098375648616315\n",
      "Coefficient of determination (Ridge) 0.6732468455387907\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "ridge = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    Ridge(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_ridge = ridge.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_ridge = mean_absolute_error(y_train, y_pred_ridge)\n",
    "print(\"Training mean absolute error (Ridge)\", mae_ridge)\n",
    "\n",
    "# Mean squared error\n",
    "mse_ridge = mean_squared_error(y_train, y_pred_ridge)\n",
    "print(\"Training mean squared error (Ridge)\", mse_ridge)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_train, y_pred_ridge))\n",
    "print(\"Training root mean squared error (Ridge)\", rmse_ridge)\n",
    "\n",
    "# R2 score\n",
    "r2_ridge = r2_score(y_train, y_pred_ridge)\n",
    "print(\"Coefficient of determination (Ridge)\", r2_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'ridge__alpha': 1, 'ridge__solver': 'saga'}\n",
      "Best Score: 0.6509778110352706\n",
      "Training mean absolute error (Ridge) 3.3611494891012974\n",
      "Training mean squared error (Ridge) 17.72310069381754\n",
      "Training root mean squared error (Ridge) 4.209881315882615\n",
      "Coefficient of determination (Ridge) 0.6732400538950838\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Ridge regression\n",
    "ridge_parameters = {\n",
    "    'ridge__alpha': [0.1, 1, 10, 100, 1000],\n",
    "    'ridge__solver': ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "ridge_cv = GridSearchCV(ridge, ridge_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "ridge_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_ridge_cv = ridge_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", ridge_cv.best_params_)\n",
    "print(\"Best Score:\", ridge_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_ridge_cv = mean_absolute_error(y_train, y_pred_ridge_cv)\n",
    "print(\"Training mean absolute error (Ridge)\", mae_ridge_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_ridge_cv = mean_squared_error(y_train, y_pred_ridge_cv)\n",
    "print(\"Training mean squared error (Ridge)\", mse_ridge_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_ridge_cv = np.sqrt(mean_squared_error(y_train, y_pred_ridge_cv))\n",
    "print(\"Training root mean squared error (Ridge)\", rmse_ridge_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_ridge_cv = r2_score(y_train, y_pred_ridge_cv)\n",
    "print(\"Coefficient of determination (Ridge)\", r2_ridge_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Lasso) 3.8676133080167543\n",
      "Training mean squared error (Lasso) 22.908404470964527\n",
      "Training root mean squared error (Lasso) 4.786272502790092\n",
      "Coefficient of determination (Lasso) 0.5776388601745506\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "lasso = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    Lasso(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_lasso = lasso.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_lasso = mean_absolute_error(y_train, y_pred_lasso)\n",
    "print(\"Training mean absolute error (Lasso)\", mae_lasso)\n",
    "\n",
    "# Mean squared error\n",
    "mse_lasso = mean_squared_error(y_train, y_pred_lasso)\n",
    "print(\"Training mean squared error (Lasso)\", mse_lasso)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_train, y_pred_lasso))\n",
    "print(\"Training root mean squared error (Lasso)\", rmse_lasso)\n",
    "\n",
    "# R2 score\n",
    "r2_lasso = r2_score(y_train, y_pred_lasso)\n",
    "print(\"Coefficient of determination (Lasso)\", r2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'lasso__alpha': 0.01}\n",
      "Best Score: 0.6508809302002576\n",
      "Training mean absolute error (Lasso) 3.3607827795791265\n",
      "Training mean squared error (Lasso) 17.72275252317157\n",
      "Training root mean squared error (Lasso) 4.2098399640807695\n",
      "Coefficient of determination (Lasso) 0.6732464731003616\n"
     ]
    }
   ],
   "source": [
    "# Grid search for Lasso model\n",
    "lasso_parameters = {'lasso__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    \n",
    "lasso_cv = GridSearchCV(lasso, lasso_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_lasso_cv = lasso_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", lasso_cv.best_params_)\n",
    "print(\"Best Score:\", lasso_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_lasso_cv = mean_absolute_error(y_train, y_pred_lasso_cv)\n",
    "print(\"Training mean absolute error (Lasso)\", mae_lasso_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_lasso_cv = mean_squared_error(y_train, y_pred_lasso_cv)\n",
    "print(\"Training mean squared error (Lasso)\", mse_lasso_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_lasso_cv = np.sqrt(mean_squared_error(y_train, y_pred_lasso_cv))\n",
    "print(\"Training root mean squared error (Lasso)\", rmse_lasso_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_lasso_cv = r2_score(y_train, y_pred_lasso_cv)\n",
    "print(\"Coefficient of determination (Lasso)\", r2_lasso_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (ElasticNet) 3.7217925323600856\n",
      "Training mean squared error (ElasticNet) 21.352749653892875\n",
      "Training root mean squared error (ElasticNet) 4.620903553840187\n",
      "Coefficient of determination (Elastic) 0.6063203924281877\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "elastic = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    ElasticNet(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "elastic.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_elastic = elastic.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_elastic = mean_absolute_error(y_train, y_pred_elastic)\n",
    "print(\"Training mean absolute error (ElasticNet)\", mae_elastic)\n",
    "\n",
    "# Mean squared error\n",
    "mse_elastic = mean_squared_error(y_train, y_pred_elastic)\n",
    "print(\"Training mean squared error (ElasticNet)\", mse_elastic)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_elastic = np.sqrt(mean_squared_error(y_train, y_pred_elastic))\n",
    "print(\"Training root mean squared error (ElasticNet)\", rmse_elastic)\n",
    "\n",
    "# R2 score\n",
    "r2_elastic = r2_score(y_train, y_pred_elastic)\n",
    "print(\"Coefficient of determination (Elastic)\", r2_elastic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.240e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.003e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.094e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.062e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.056e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.260e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.028e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.115e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.084e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.081e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.431e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.292e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.270e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.288e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.337e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.173e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.181e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.188e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.291e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.344e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.006e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.031e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.344e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.006e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.432e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.269e+03, tolerance: 1.340e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.809e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.410e+03, tolerance: 1.354e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.809e+03, tolerance: 1.233e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.851e+03, tolerance: 1.232e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.890e+03, tolerance: 1.261e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'elasticnet__alpha': 0.01, 'elasticnet__l1_ratio': 0.0, 'elasticnet__selection': 'cyclic'}\n",
      "Best Score: 0.651054776153045\n",
      "Training mean absolute error (ElasticNet) 3.3626317420104828\n",
      "Training mean squared error (ElasticNet) 17.726639497392146\n",
      "Training root mean squared error (ElasticNet) 4.2103015922130975\n",
      "Coefficient of determination (Elastic) 0.6731748091455735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\predicting_height\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.654e+03, tolerance: 1.605e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    }
   ],
   "source": [
    "# Grid Search CV for ElasticNet\n",
    "elastic_parameters = {\n",
    "    'elasticnet__alpha': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'elasticnet__l1_ratio': [0.0, 0.25, 0.5, 0.75, 1.0],\n",
    "    'elasticnet__selection': ['cyclic', 'random']\n",
    "}\n",
    "\n",
    "elastic_cv = GridSearchCV(elastic, elastic_parameters, cv=5)\n",
    "\n",
    "# Fit model\n",
    "elastic_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_elastic_cv = elastic_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", elastic_cv.best_params_)\n",
    "print(\"Best Score:\", elastic_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_elastic_cv = mean_absolute_error(y_train, y_pred_elastic_cv)\n",
    "print(\"Training mean absolute error (ElasticNet)\", mae_elastic_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_elastic_cv = mean_squared_error(y_train, y_pred_elastic_cv)\n",
    "print(\"Training mean squared error (ElasticNet)\", mse_elastic_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_elastic_cv = np.sqrt(mean_squared_error(y_train, y_pred_elastic_cv))\n",
    "print(\"Training root mean squared error (ElasticNet)\", rmse_elastic_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_elastic_cv = r2_score(y_train, y_pred_elastic_cv)\n",
    "print(\"Coefficient of determination (Elastic)\", r2_elastic_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training mean absolute error (Random Forest) 1.6993276034963478\n",
      "Training mean squared error (Random Forest) 5.15115593898109\n",
      "Training root mean squared error (Random Forest) 2.269615813079626\n",
      "Coefficient of determination (Random Forest) 0.9050283883120611\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "rf_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    RandomForestRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_rf = rf_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_rf = mean_absolute_error(y_train, y_pred_rf)\n",
    "print(\"Training mean absolute error (Random Forest)\", mae_rf)\n",
    "\n",
    "# Mean squared error\n",
    "mse_rf = mean_squared_error(y_train, y_pred_rf)\n",
    "print(\"Training mean squared error (Random Forest)\", mse_rf)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_train, y_pred_rf))\n",
    "print(\"Training root mean squared error (Random Forest)\", rmse_rf)\n",
    "\n",
    "# R2 score\n",
    "r2_rf = r2_score(y_train, y_pred_rf)\n",
    "print(\"Coefficient of determination (Random Forest)\", r2_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV with random forest\n",
    "\n",
    "rf_parameters = {\n",
    "    'randomforestregressor__n_estimators': [100, 200, 300],\n",
    "    'randomforestregressor__max_features': ['sqrt', 'log2', None],\n",
    "    'randomforestregressor__max_depth': [10, 20, 30, None],\n",
    "    'randomforestregressor__max_leaf_nodes': [None, 10, 20, 50, 100],\n",
    "    'randomforestregressor__criterion': ['squared_error', 'absolute_error', 'friedman_mse', 'poisson']\n",
    "}\n",
    "\n",
    "rf_cv = GridSearchCV(rf_model, rf_parameters, cv=5, n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "rf_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_rf_cv = rf_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", rf_cv.best_params_)\n",
    "print(\"Best Score:\", rf_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_rf_cv = mean_absolute_error(y_train, y_pred_rf_cv)\n",
    "print(\"Training mean absolute error (Random Forest)\", mae_rf_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_rf_cv = mean_squared_error(y_train, y_pred_rf_cv)\n",
    "print(\"Training mean squared error (Random Forest)\", mse_rf_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_rf_cv = np.sqrt(mean_squared_error(y_train, y_pred_rf_cv))\n",
    "print(\"Training root mean squared error (Random Forest)\", rmse_rf_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_rf_cv = r2_score(y_train, y_pred_rf_cv)\n",
    "print(\"Coefficient of determination (Random Forest)\", r2_rf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "gb_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    GradientBoostingRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_gb = gb_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_gb = mean_absolute_error(y_train, y_pred_gb)\n",
    "print(\"Training mean absolute error (Gradient Boosting)\", mae_gb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_gb = mean_squared_error(y_train, y_pred_gb)\n",
    "print(\"Training mean squared error (Gradient Boosting)\", mse_gb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_train, y_pred_gb))\n",
    "print(\"Training root mean squared error (Gradient Boosting)\", rmse_gb)\n",
    "\n",
    "# R2 score\n",
    "r2_gb = r2_score(y_train, y_pred_gb)\n",
    "print(\"Coefficient of determination (Gradient Boosting)\", r2_gb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV with gradient boosting\n",
    "\n",
    "gb_parameters = {\n",
    "    'gradientboostingregressor__n_estimators': [100, 200, 300],\n",
    "    'gradientboostingregressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'gradientboostingregressor__max_depth': [3, 4, 5],\n",
    "    'gradientboostingregressor__subsample': [0.8, 0.9, 1.0],\n",
    "    'gradientboostingregressor__loss': ['squared_error', 'absolute_error', 'huber', 'quantile']\n",
    "}\n",
    "\n",
    "\n",
    "gb_cv = GridSearchCV(gb_model, gb_parameters, cv=5, n_jobs = -1)\n",
    "\n",
    "# Fit model\n",
    "gb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_gb_cv = gb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", gb_cv.best_params_)\n",
    "print(\"Best Score:\", gb_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_gb_cv = mean_absolute_error(y_train, y_pred_gb_cv)\n",
    "print(\"Training mean absolute error (Gradient Boosting)\", mae_gb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_gb_cv = mean_squared_error(y_train, y_pred_gb_cv)\n",
    "print(\"Training mean squared error (Gradient Boosting)\", mse_gb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_gb_cv = np.sqrt(mean_squared_error(y_train, y_pred_gb_cv))\n",
    "print(\"Training root mean squared error (Gradient Boosting)\", rmse_gb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_gb_cv = r2_score(y_train, y_pred_gb_cv)\n",
    "print(\"Coefficient of determination (Gradient Boosting)\", r2_gb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "xgb_model = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    XGBRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_xgb = xgb_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_xgb = mean_absolute_error(y_train, y_pred_xgb)\n",
    "print(\"Training mean absolute error (Extreme Gradient)\", mae_xgb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_xgb = mean_squared_error(y_train, y_pred_xgb)\n",
    "print(\"Training mean squared error (Extreme Gradient)\", mse_xgb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_train, y_pred_xgb))\n",
    "print(\"Training root mean squared error (Extreme Gradient)\", rmse_xgb)\n",
    "\n",
    "# R2 score\n",
    "r2_xgb = r2_score(y_train, y_pred_xgb)\n",
    "print(\"Coefficient of determination (Extreme Gradien)\", r2_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV with extreme gradient boosting\n",
    "xgb_parameters = {\n",
    "    'xgbregressor__n_estimators': [100, 200, 300],\n",
    "    'xgbregressor__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgbregressor__max_depth': [3, 4, 5],\n",
    "    'xgbregressor__subsample': [0.8, 0.9, 1.0],\n",
    "    'xgbregressor__colsample_bytree': [0.3, 0.7, 1.0],\n",
    "    'xgbregressor__min_child_weight': [1, 3, 5],\n",
    "    'xgbregressor__gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "xgb_cv = GridSearchCV(xgb_model, xgb_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "xgb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_xgb_cv = xgb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", xgb_cv.best_params_)\n",
    "print(\"Best Score:\", xgb_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_xgb_cv = mean_absolute_error(y_train, y_pred_xgb_cv)\n",
    "print(\"Training mean absolute error (Extreme Gradient)\", mae_xgb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_xgb_cv = mean_squared_error(y_train, y_pred_xgb_cv)\n",
    "print(\"Training mean squared error (Extreme Gradient)\", mse_xgb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_xgb_cv = np.sqrt(mean_squared_error(y_train, y_pred_xgb_cv))\n",
    "print(\"Training root mean squared error (Extreme Gradient)\", rmse_xgb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_xgb_cv = r2_score(y_train, y_pred_xgb_cv)\n",
    "print(\"Coefficient of determination (Extreme Gradient)\", r2_xgb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tree_model = make_pipeline(\n",
    "    OrdinalEncoder(),\n",
    "    DecisionTreeRegressor(random_state=42)\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "tree_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_tree = tree_model.predict(X_train)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_tree = mean_absolute_error(y_train, y_pred_tree)\n",
    "print(\"Training mean absolute error (Decision Tree)\", mae_tree)\n",
    "\n",
    "# Mean squared error\n",
    "mse_tree = mean_squared_error(y_train, y_pred_tree)\n",
    "print(\"Training mean squared error (Decision Tree)\", mse_tree)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_tree = np.sqrt(mean_squared_error(y_train, y_pred_tree))\n",
    "print(\"Training root mean squared error (Decision Tree)\", rmse_tree)\n",
    "\n",
    "# R2 score\n",
    "r2_tree = r2_score(y_train, y_pred_tree)\n",
    "print(\"Coefficient of determination (Decision Tree)\", r2_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV with decision tree\n",
    "tree_parameters = {\n",
    "    'decisiontreeregressor__splitter': ['best', 'random'],\n",
    "    'decisiontreeregressor__max_depth': [None, 10, 20, 30, 40, 50],\n",
    "    'decisiontreeregressor__min_samples_split': [2, 5, 10],\n",
    "    'decisiontreeregressor__min_samples_leaf': [1, 2, 4],\n",
    "    'decisiontreeregressor__max_features': ['auto', 'sqrt', 'log2', None],\n",
    "    'decisiontreeregressor__max_leaf_nodes': [None, 10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "\n",
    "tree_cv = GridSearchCV(tree_model, tree_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "tree_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_tree_cv = tree_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", tree_cv.best_params_)\n",
    "print(\"Best Score:\", tree_cv.best_score_)\n",
    "\n",
    "# Evaluate baseline predictions\n",
    "# Mean absolute error\n",
    "mae_tree_cv = mean_absolute_error(y_train, y_pred_tree_cv)\n",
    "print(\"Training mean absolute error (Decision Tree)\", mae_tree_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_tree_cv = mean_squared_error(y_train, y_pred_tree_cv)\n",
    "print(\"Training mean squared error (Decision Tree)\", mse_tree_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_tree_cv = np.sqrt(mean_squared_error(y_train, y_pred_tree_cv))\n",
    "print(\"Training root mean squared error (Decision Tree)\", rmse_tree_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_tree_cv = r2_score(y_train, y_pred_tree_cv)\n",
    "print(\"Coefficient of determination (Decision Tree)\", r2_tree_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "hb = make_pipeline(\n",
    "    OneHotEncoder(),\n",
    "    HuberRegressor()\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "hb.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_hb = hb.predict(X_train)\n",
    "\n",
    "# Evaluate predictions\n",
    "# Mean absolute error\n",
    "mae_hb = mean_absolute_error(y_train, y_pred_hb)\n",
    "print(\"Training mean absolute error (Huber)\", mae_hb)\n",
    "\n",
    "# Mean squared error\n",
    "mse_hb = mean_squared_error(y_train, y_pred_hb)\n",
    "print(\"Training mean squared error (Huber)\", mse_hb)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_hb = np.sqrt(mean_squared_error(y_train, y_pred_hb))\n",
    "print(\"Training root mean squared error (Huber)\", rmse_hb)\n",
    "\n",
    "# R2 score\n",
    "r2_hb = r2_score(y_train, y_pred_hb)\n",
    "print(\"Coefficient of determination (Huber)\", r2_hb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search CV with Huber regressor\n",
    "hb_parameters = {\n",
    "    'huberregressor__epsilon': [1.35, 1.5, 1.75, 2.0],\n",
    "    'huberregressor__alpha': [0.0001, 0.001, 0.01, 0.1, 1],\n",
    "    'huberregressor__max_iter': [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "hb_cv = GridSearchCV(hb, hb_parameters, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit model\n",
    "hb_cv.fit(X_train, y_train)\n",
    "\n",
    "# Predict for training data\n",
    "y_pred_hb_cv = hb_cv.predict(X_train)\n",
    "\n",
    "# Best parameters and best score from cross validation\n",
    "print(\"Best Parameters:\", hb_cv.best_params_)\n",
    "print(\"Best Score:\", hb_cv.best_score_)\n",
    "\n",
    "# Evaluate predictions\n",
    "# Mean absolute error\n",
    "mae_hb_cv = mean_absolute_error(y_train, y_pred_hb_cv)\n",
    "print(\"Training mean absolute error (Huber)\", mae_hb_cv)\n",
    "\n",
    "# Mean squared error\n",
    "mse_hb_cv = mean_squared_error(y_train, y_pred_hb_cv)\n",
    "print(\"Training mean squared error (Huber)\", mse_hb_cv)\n",
    "\n",
    "# Root mean squared error\n",
    "rmse_hb_cv = np.sqrt(mean_squared_error(y_train, y_pred_hb_cv))\n",
    "print(\"Training root mean squared error (Huber)\", rmse_hb_cv)\n",
    "\n",
    "# R2 score\n",
    "r2_hb_cv = r2_score(y_train, y_pred_hb_cv)\n",
    "print(\"Coefficient of determination (Huber)\", r2_hb_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(models, x, y):\n",
    "    \"\"\"\n",
    "    Evaluate models on test data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    models: dict, Dictionary with instances of models\n",
    "    x: DataFrame, Feature matrix for test data\n",
    "    y: DataFrame, Target vector for test data\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    Results of model evaluation: DataFrame, Name of model, mean absolute error, mean squared error, \n",
    "    root mean squared error, coefficient of determination\n",
    "    \"\"\"\n",
    "    # Empty list to hold results\n",
    "    test_results = []\n",
    "    \n",
    "    # Loop through model dictionary\n",
    "    for name, model in models.items():\n",
    "        # Create a prediction series with model\n",
    "        y_pred_test = model.predict(x)\n",
    "        \n",
    "        # Evaluate mean absolute error\n",
    "        mae = mean_absolute_error(y, y_pred_test)\n",
    "        \n",
    "        # Evaluate mean squared error\n",
    "        mse = mean_squared_error(y, y_pred_test)\n",
    "        \n",
    "        # Evaluate root mean squared error\n",
    "        rmse = np.sqrt(mean_squared_error(y, y_pred_test))\n",
    "        \n",
    "        # Evaluate coefficient of determination\n",
    "        r2 = r2_score(y, y_pred_test)\n",
    "        \n",
    "        # Append name of model and model metrics\n",
    "        test_results.append(name, mae, mse, rmse, r2)\n",
    "\n",
    "    # Create a dataframe with test result list\n",
    "    results_df = pd.DataFrame(test_results, columns=[\"Models\", \"MAE\", \"MSE\", \"RMSE\", \"R2\"])\n",
    "    \n",
    "    # Return data frame of test results\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"Linear regression\": lr_model,\n",
    "    \"Ridge regression\": ridge_cv,\n",
    "    \"Lasso\": lasso_cv,\n",
    "    \"Elastic net\": elastic_cv,\n",
    "    \"Random Forest\": rf_cv,\n",
    "    \"Gradient Boosting\": gb_cv,\n",
    "    \"XGB\": xgb_cv,\n",
    "    \"Decision tree\": tree_cv,\n",
    "    \"Huber regressor\": hb\n",
    "}\n",
    "\n",
    "# Call test model with args: models, X_test, y_test\n",
    "test_model(models, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "predicting_height",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
